{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhptidykpIei",
        "outputId": "751c2623-f899-46f4-b488-0a3d93fed2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from time import time\n",
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn import metrics\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from tables.path import keyword\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import  keras\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "_CAT = 'category'\n",
        "_MSG = 'message'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIyZZrKlI4qh"
      },
      "source": [
        "# Curso de Aprendizaje Automático\n",
        "# Trabajo Practico 2: Redes Recurrentes y Representaciones Incrustadas\n",
        "\n",
        "**Escuela de Ingeniería en Computación | Instituto Tecnológico de Costa Rica**\n",
        "\n",
        "Realizado por:\n",
        "\n",
        "*   Victoria Orozco\n",
        "*   Ignacio Barquero\n",
        "*   Esteban Villalobos\n",
        "\n",
        "Fecha de entrega:\n",
        "\n",
        "* 20 de Noviembre de 2022\n",
        "\n",
        "Tipo de entrega:\n",
        "\n",
        "* Digital, por medio de la plataforma TEC-digital.\n",
        "\n",
        "Modo de trabajo\n",
        "\n",
        "* Grupos de 2/3 personas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPPXJYc6Xr2l"
      },
      "source": [
        "\n",
        "## Data preprocessing\n",
        "The class Preprocessing loads the specific dataset and makes the data partitions. It also converts the input text to indices, in order to feed the embedding layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ni1ynnCWLYB"
      },
      "outputs": [],
      "source": [
        "def _preprocesar_oracion_1(sentence):\n",
        "\t''' Removes capitalization and stopworkds '''\n",
        "\n",
        "\t# Tokenize words while ignoring punctuation\n",
        "\ttokeniser = RegexpTokenizer(r'\\w+')\n",
        "\ttokens = tokeniser.tokenize(sentence)\n",
        "\tkeywords = [token.lower() for token in tokens\n",
        "\t            if token not in stopwords.words('english')]\n",
        "\tpreprocessed_sentence = \" \".join(keywords)\n",
        "\treturn preprocessed_sentence\n",
        "\n",
        "\n",
        "def _preprocesar_oracion_2(sentece):\n",
        "\t''' Removes capitalization, stopworkds and lematizes'''\n",
        "\n",
        "\t# Tokenize words while ignoring punctuation\n",
        "\ttokeniser = RegexpTokenizer(r'\\w+')\n",
        "\ttokens = tokeniser.tokenize(sentece)\n",
        "\n",
        "\t# Lowercase and remove stopwords\n",
        "\tfiltered_tokens = [token.lower() for token in tokens\n",
        "\t                   if token not in stopwords.words('english')]\n",
        "\n",
        "\t# Lowercase and lemmatise\n",
        "\tlemmatiser = WordNetLemmatizer()\n",
        "\tlemmas = [lemmatiser.lemmatize(token, pos='v') for token in filtered_tokens]\n",
        "\n",
        "\tpreprocessed_sentence = \" \".join(lemmas)\n",
        "\treturn preprocessed_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOdbtKjhYefw"
      },
      "source": [
        "Excercize 1.a.1: Compare preprocessing versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOD20CkGXmh-"
      },
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "\t''' Usage example\n",
        "\tInput doc:\n",
        "\t\t['URGENT! Your Mobile number has been awarded a <UKP>2000 prize GUARANTEED. Call 09061790125 from landline. Claim 3030. Valid 12hrs only 150ppm']\n",
        "\n",
        "\tOutput:\n",
        "\t\tpreprocesar_documento_1:\n",
        "\t\t\t['urgent your mobile number awarded ukp 2000 prize guaranteed call 09061790125 landline claim 3030 valid 12hrs 150ppm']\n",
        "\n",
        "\t\tProcesar_documento_2:\n",
        "\t\t\t['urgent mobile number award ukp 2000 prize guarantee call 09061790125 landline claim 3030 valid 12hrs 150ppm']\n",
        "\t'''\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.data = 'SMSSpamCollection'\n",
        "    #maximum length for each sequence, CORRECT\n",
        "\t\tself.max_len = 200\n",
        "    #Maximum number of words in the dictionary\n",
        "\t\tself.max_words = 200\n",
        "\t\t#percentage of test data\n",
        "\t\tself.test_size = 0.2\n",
        "\n",
        "\tdef load_data(self, drop_na=False):\n",
        "\t\t\"\"\"\n",
        "\t\tLoads and splits the data\n",
        "\t\t\"\"\"\n",
        "\t\t# load training and test data\n",
        "\t\tdf = pd.read_csv(self.data, sep=\"\\t\", names = [_CAT, _MSG], header = None)\n",
        "\t\t# print(df)\n",
        "\n",
        "\t\t# Replace dependent variable to a int value: 0 = not_spam, 1 = spam\n",
        "\t\tdf.category = df.category.map({'ham': 0., 'spam': 1.})\n",
        "\t\tdf.astype({_CAT:'float'})\n",
        "\t\tprint(df)\n",
        "\n",
        "\t\t# extract input and labels\n",
        "\t\tX = df['message'].values\n",
        "\t\tY = df['category'].values\n",
        "\t\t# create train/test split using sklearn\n",
        "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n",
        "\n",
        "\tdef preprocesar_documento_1(self):\n",
        "\t\t\"\"\" Preprocess training corpus version 1 \"\"\"\n",
        "\n",
        "\t\tfor i in range (0, len(self.x_train)):\n",
        "\t\t\tself.x_train[i] = _preprocesar_oracion_1(self.x_train[i])\n",
        "\n",
        "\tdef preprocesar_documento_2(self):\n",
        "\t\t\"\"\" Preprocess training corpus version 2 \"\"\"\n",
        "\n",
        "\t\tfor i in range (0, len(self.x_train)):\n",
        "\t\t\tself.x_train[i] = _preprocesar_oracion_2(self.x_train[i])\n",
        "\n",
        "\tdef prepare_tokens(self, version=2):\n",
        "\t\t\"\"\"\n",
        "\t\tTokenizes the input text\n",
        "\t\t\"\"\"\n",
        "\t\t#tokenize the input text\n",
        "\t\tif version == 1:\n",
        "\t\t\tself.preprocesar_documento_1()\n",
        "\t\telse:\n",
        "\t\t\tself.preprocesar_documento_2()\n",
        "\n",
        "\t\tself.tokens = Tokenizer(num_words=self.max_words)\n",
        "\t\tself.tokens.fit_on_texts(self.x_train)\n",
        "\n",
        "\tdef sequence_to_token(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tConverts the input sequence of strings to a sequence of integers\n",
        "\t\t\"\"\"\n",
        "\t\t#transform the token list to a sequence of integers\n",
        "\t\tsequences = self.tokens.texts_to_sequences(x)\n",
        "\t  #add padding using the maximum length specified\n",
        "\t\treturn keras.utils.pad_sequences(sequences, maxlen=self.max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHoxTZp1VYUE",
        "outputId": "eb36cd42-7ba3-498b-b3a1-3ba6a5c2959c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading\n",
            "      category                                            message\n",
            "0          0.0  Go until jurong point, crazy.. Available only ...\n",
            "1          0.0                      Ok lar... Joking wif u oni...\n",
            "2          1.0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3          0.0  U dun say so early hor... U c already then say...\n",
            "4          0.0  Nah I don't think he goes to usf, he lives aro...\n",
            "...        ...                                                ...\n",
            "5567       1.0  This is the 2nd time we have tried 2 contact u...\n",
            "5568       0.0               Will ü b going to esplanade fr home?\n",
            "5569       0.0  Pity, * was in mood for that. So...any other s...\n",
            "5570       0.0  The guy did some bitching but I acted like i'd...\n",
            "5571       0.0                         Rofl. Its true to its name\n",
            "\n",
            "[5572 rows x 2 columns]\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Initial sentence(random):\t No idea, I guess we'll work that out an hour after we're supposed to leave since as usual nobody has any interest in figuring shit out before the last second\n",
            "Preprocessed sentence v1:\t no idea i guess work hour supposed leave since usual nobody interest figuring shit last second\n",
            "Preprocessed sentence v2:\t no idea i guess work hour suppose leave since usual nobody interest figure shit last second\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Example\n",
            "Initial sentence:\t\t Wow. I never realized that you were so embarassed by your accomodations. I thought you liked it, since i was doing the best i could and you always seemed so happy about \"the cave\". I'm sorry I didn't and don't have more to give.\n",
            "Preprocessed sentence v1:\t wow i never realized embarassed accomodations i thought liked since best could always seemed happy cave i sorry i give\n",
            "Preprocessed sentence v2:\t wow i never realize embarassed accomodations i think like since best could always seem happy cave i sorry i give\n"
          ]
        }
      ],
      "source": [
        "p = Preprocessing()\n",
        "print('Data loading')\n",
        "p.load_data()\n",
        "print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n')\n",
        "sent = p.x_train[50]\n",
        "print('Initial sentence(random):\\t', sent)\n",
        "print('Preprocessed sentence v1:\\t', _preprocesar_oracion_1(sent))\n",
        "print('Preprocessed sentence v2:\\t', _preprocesar_oracion_2(sent))\n",
        "\n",
        "print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n')\n",
        "print('Example')\n",
        "sent = \"Wow. I never realized that you were so embarassed by your accomodations. \" \\\n",
        "       \"I thought you liked it, since i was doing the best i could and you always seemed so happy about \\\"the cave\\\". \"\\\n",
        "       \"I'm sorry I didn't and don't have more to give.\"\n",
        "print('Initial sentence:\\t\\t', sent)\n",
        "print('Preprocessed sentence v1:\\t', _preprocesar_oracion_1(sent))\n",
        "print('Preprocessed sentence v2:\\t', _preprocesar_oracion_2(sent))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9X1iJauDI2U"
      },
      "source": [
        "## Model\n",
        "Creates the LSTM model. The hidden state $h$ and cell $c$ are initialized with noise. The LSTM receives the entire sequence of embeddings.\n",
        "An Embedding layer is trained in order to learn the data representations.\n",
        "At the top of the model, a fully connected model is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYwAPjZZzSuO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTM_Classifier(nn.ModuleList):\n",
        "\n",
        "\tdef __init__(self, batch_size = 64, hidden_dim = 20, lstm_layers = 2,\n",
        "\t             max_words = 200):\n",
        "\t\t\"\"\"\n",
        "\t\tparam batch_size: batch size for training data\n",
        "\t\tparam hidden_dim: number of hidden units used in the LSTM and the\n",
        "\t\t\t\t\t\t\t\t\t\t  Embedding layer\n",
        "\t\tparam lstm_layers: number of lstm_layers\n",
        "\t\tparam max_words: maximum sentence length\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(LSTM_Classifier, self).__init__()\n",
        "\t\t#batch size during training\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\t#number of hidden units in the LSTM layer\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\t#Number of LSTM layers\n",
        "\t\tself.LSTM_layers = lstm_layers\n",
        "\t\tself.input_size = max_words # embedding dimension\n",
        "\n",
        "\t\tself.dropout = nn.Dropout(0.5)\n",
        "\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
        "\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
        "\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
        "\t\tself.fc2 = nn.Linear(257, 1)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tForward pass\n",
        "\t\tparam x: model input\n",
        "\t\t\"\"\"\n",
        "\t\t#it starts with noisy estimations of h and c\n",
        "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "\t\t#Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution.\n",
        "\t\t#The resulting tensor will have values sampled from \\mathcal{N}(0, \\text{std}^2)N(0,std)\n",
        "\t\ttorch.nn.init.xavier_normal_(h)\n",
        "\t\ttorch.nn.init.xavier_normal_(c)\n",
        "\t\t#print(\"x shape \", x.shape)\n",
        "\t\t#print(\"embedding \", self.embedding)\n",
        "\t\tout = self.embedding(x)\n",
        "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
        "\t\tout = self.dropout(out)\n",
        "\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n",
        "\t\tout = self.dropout(out)\n",
        "\t  #sigmoid activation function\n",
        "\t\tout = torch.sigmoid(self.fc2(out))\n",
        "\n",
        "\t\treturn out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjKwvaHCbW5P"
      },
      "source": [
        "## Data iterator\n",
        "In order to get ready the training phase, first, we need to prepare the way how the sequences will be fed to the model. For this purpose, PyTorch provides two very useful classes: Dataset and DataLoader. The aim of Dataset class is to provide an easy way to iterate over a dataset by batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbryvCtmbUTP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class DatasetMaper(Dataset):\n",
        "\t'''\n",
        "\tHandles batches of dataset\n",
        "\t'''\n",
        "\tdef __init__(self, x, y):\n",
        "\t\t\"\"\"\n",
        "\t\tInits the dataset mapper\n",
        "\t\t\"\"\"\n",
        "\t\tself.x = x\n",
        "\t\tself.y = y\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the length of the dataset\n",
        "\t\t\"\"\"\n",
        "\t\treturn len(self.x)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\t\"\"\"\n",
        "\t\tFetches a specific item by id\n",
        "\t\t\"\"\"\n",
        "\t\treturn self.x[idx], self.y[idx]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QERtKkG1a2CY"
      },
      "source": [
        "## Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRxukrIia4XI"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(batch_size = 64, preprocess_version=2):\n",
        "  preprocessor = Preprocessing()\n",
        "  #load the data\n",
        "  preprocessor.load_data()\n",
        "\n",
        "  #tokenize the text\n",
        "  preprocessor.prepare_tokens(version=preprocess_version)\n",
        "  raw_x_train = preprocessor.x_train\n",
        "  raw_x_test = preprocessor.x_test\n",
        "  y_train = preprocessor.y_train\n",
        "  y_test = preprocessor.y_test\n",
        "\n",
        "  #convert sequence of strings to tokens\n",
        "  x_train = preprocessor.sequence_to_token(raw_x_train)\n",
        "  x_test = preprocessor.sequence_to_token(raw_x_test)\n",
        "\n",
        "  #create data loaders\n",
        "  training_set = DatasetMaper(x_train, y_train)\n",
        "  test_set = DatasetMaper(x_test, y_test)\n",
        "  loader_training = DataLoader(training_set, batch_size=batch_size)\n",
        "  loader_test = DataLoader(test_set)\n",
        "  return loader_training, loader_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTVKjxt2sd3z",
        "outputId": "73a06ddf-7d1b-48cf-c2b0-e4ebdd9af567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      category                                            message\n",
            "0          0.0  Go until jurong point, crazy.. Available only ...\n",
            "1          0.0                      Ok lar... Joking wif u oni...\n",
            "2          1.0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3          0.0  U dun say so early hor... U c already then say...\n",
            "4          0.0  Nah I don't think he goes to usf, he lives aro...\n",
            "...        ...                                                ...\n",
            "5567       1.0  This is the 2nd time we have tried 2 contact u...\n",
            "5568       0.0               Will ü b going to esplanade fr home?\n",
            "5569       0.0  Pity, * was in mood for that. So...any other s...\n",
            "5570       0.0  The guy did some bitching but I acted like i'd...\n",
            "5571       0.0                         Rofl. Its true to its name\n",
            "\n",
            "[5572 rows x 2 columns]\n",
            "      category                                            message\n",
            "0          0.0  Go until jurong point, crazy.. Available only ...\n",
            "1          0.0                      Ok lar... Joking wif u oni...\n",
            "2          1.0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3          0.0  U dun say so early hor... U c already then say...\n",
            "4          0.0  Nah I don't think he goes to usf, he lives aro...\n",
            "...        ...                                                ...\n",
            "5567       1.0  This is the 2nd time we have tried 2 contact u...\n",
            "5568       0.0               Will ü b going to esplanade fr home?\n",
            "5569       0.0  Pity, * was in mood for that. So...any other s...\n",
            "5570       0.0  The guy did some bitching but I acted like i'd...\n",
            "5571       0.0                         Rofl. Its true to its name\n",
            "\n",
            "[5572 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "loader_training_v2, loader_test_v2 = create_data_loaders(preprocess_version=2)\n",
        "loader_training_v1, loader_test_v1 = create_data_loaders(preprocess_version=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl-8EoAGoCVi"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lz0DBA9oML6"
      },
      "source": [
        "Accuracy evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "744SFPymoDMN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_accuray(y_pred, y_gt):\n",
        "  return accuracy_score(y_pred, y_gt)\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader_test):\n",
        "\n",
        "  predictions = []\n",
        "  accuracies = []\n",
        "    # The model is turned in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "      # Skipping gradients update\n",
        "  with torch.no_grad():\n",
        "\n",
        "            # Iterate over the DataLoader object\n",
        "    for x_batch, y_batch in loader_test:\n",
        "      #print(\"batch\")\n",
        "      x = x_batch.type(torch.LongTensor)\n",
        "      y = y_batch.type(torch.FloatTensor)\n",
        "\n",
        "                  # Feed the model\n",
        "      y_pred = model(x)\n",
        "      y_pred = torch.round(y_pred).flatten()\n",
        "      # print(\"y_pred: \", y_pred)\n",
        "      # Save prediction\n",
        "      predictions += list(y_pred.detach().numpy())\n",
        "      acc_batch = accuracy_score(y_pred, y)\n",
        "      accuracies += [acc_batch]\n",
        "  return np.array(accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQqpD4E5oSJn"
      },
      "source": [
        "Train the model using the dataset loader for the training partition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1aMOQb-oHi6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, loader_training, loader_test, epochs = 10,\n",
        "                learning_rate = 0.01):\n",
        "\n",
        "  # Defines a RMSprop optimizer to update the parameters\n",
        "  optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # model in training mode\n",
        "    model.train()\n",
        "    loss_dataset = 0\n",
        "    for x_batch, y_batch in loader_training:\n",
        "      #print(\"x_batch \\n \", x_batch)\n",
        "      #print(\"y batch \\n\", y_batch)\n",
        "      x = x_batch.type(torch.LongTensor)\n",
        "      y = y_batch.type(torch.FloatTensor)\n",
        "      # Feed the model the entire sequence and get output \"y_pred\"\n",
        "      y_pred = model(x).flatten()\n",
        "      #print(\"y\\n\", y)\n",
        "      #print(\"y pred \", y_pred)\n",
        "      # Calculate loss\n",
        "      loss = F.binary_cross_entropy(y_pred, y)\n",
        "\n",
        "      # The gradientes are calculated\n",
        "      # i.e. derivates are calculated\n",
        "      loss.backward()\n",
        "\n",
        "      # Each parameter is updated\n",
        "      # with torch.no_grad():\n",
        "      #     a -= lr * a.grad\n",
        "      #     b -= lr * b.grad\n",
        "      optimizer.step()\n",
        "      # Take the gradients to zero!\n",
        "      # a.grad.zero_()\n",
        "      # b.grad.zero_()\n",
        "      optimizer.zero_grad()\n",
        "      loss_dataset += loss\n",
        "    accuracies = evaluate_model(model, loader_test)\n",
        "    print(\"Epoch \", epoch, \" Loss training : \", loss_dataset.item(), \" Accuracy test: \", accuracies.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5RmqpHpziUF"
      },
      "source": [
        "Test: D = 20, preprocess version 2, 10 models test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H99-ABS-sp9D",
        "outputId": "90a27c44-8126-43cb-c901-67a323191219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model # 0\n",
            "Epoch  0  Loss training :  19.57164192199707  Accuracy test:  0.9596412556053812\n",
            "Epoch  1  Loss training :  6.532021522521973  Accuracy test:  0.9659192825112107\n",
            "Epoch  2  Loss training :  5.148623466491699  Accuracy test:  0.967713004484305\n",
            "Epoch  3  Loss training :  4.374024391174316  Accuracy test:  0.9560538116591928\n",
            "Epoch  4  Loss training :  3.4256131649017334  Accuracy test:  0.9542600896860987\n",
            "Epoch  5  Loss training :  3.024905204772949  Accuracy test:  0.9659192825112107\n",
            "Epoch  6  Loss training :  2.7564826011657715  Accuracy test:  0.9641255605381166\n",
            "Epoch  7  Loss training :  1.9829388856887817  Accuracy test:  0.9605381165919282\n",
            "Epoch  8  Loss training :  1.7899994850158691  Accuracy test:  0.9542600896860987\n",
            "Epoch  9  Loss training :  1.202175498008728  Accuracy test:  0.9614349775784753\n",
            "average accuracy model #0: 0.9614349775784753\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 1\n",
            "Epoch  0  Loss training :  14.98291301727295  Accuracy test:  0.9587443946188341\n",
            "Epoch  1  Loss training :  5.74479341506958  Accuracy test:  0.9452914798206278\n",
            "Epoch  2  Loss training :  4.448873043060303  Accuracy test:  0.9479820627802691\n",
            "Epoch  3  Loss training :  3.70823073387146  Accuracy test:  0.9488789237668162\n",
            "Epoch  4  Loss training :  2.8232414722442627  Accuracy test:  0.9345291479820628\n",
            "Epoch  5  Loss training :  18.111970901489258  Accuracy test:  0.9434977578475336\n",
            "Epoch  6  Loss training :  6.011516571044922  Accuracy test:  0.9641255605381166\n",
            "Epoch  7  Loss training :  4.025938034057617  Accuracy test:  0.9623318385650225\n",
            "Epoch  8  Loss training :  3.392911195755005  Accuracy test:  0.9443946188340807\n",
            "Epoch  9  Loss training :  2.7800400257110596  Accuracy test:  0.9488789237668162\n",
            "average accuracy model #1: 0.9488789237668162\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 2\n",
            "Epoch  0  Loss training :  16.166078567504883  Accuracy test:  0.9605381165919282\n",
            "Epoch  1  Loss training :  6.146551609039307  Accuracy test:  0.9524663677130045\n",
            "Epoch  2  Loss training :  4.148004531860352  Accuracy test:  0.9506726457399103\n",
            "Epoch  3  Loss training :  3.38273024559021  Accuracy test:  0.9363228699551569\n",
            "Epoch  4  Loss training :  2.931633234024048  Accuracy test:  0.9399103139013453\n",
            "Epoch  5  Loss training :  2.521327495574951  Accuracy test:  0.9506726457399103\n",
            "Epoch  6  Loss training :  1.896902084350586  Accuracy test:  0.9461883408071748\n",
            "Epoch  7  Loss training :  1.6688318252563477  Accuracy test:  0.9461883408071748\n",
            "Epoch  8  Loss training :  2.3147571086883545  Accuracy test:  0.9434977578475336\n",
            "Epoch  9  Loss training :  2.3013243675231934  Accuracy test:  0.9479820627802691\n",
            "average accuracy model #2: 0.9479820627802691\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 3\n",
            "Epoch  0  Loss training :  15.392121315002441  Accuracy test:  0.967713004484305\n",
            "Epoch  1  Loss training :  5.248407363891602  Accuracy test:  0.9587443946188341\n",
            "Epoch  2  Loss training :  4.201364040374756  Accuracy test:  0.9641255605381166\n",
            "Epoch  3  Loss training :  3.5092923641204834  Accuracy test:  0.9533632286995516\n",
            "Epoch  4  Loss training :  2.7421374320983887  Accuracy test:  0.9560538116591928\n",
            "Epoch  5  Loss training :  3.0412514209747314  Accuracy test:  0.9641255605381166\n",
            "Epoch  6  Loss training :  2.1330642700195312  Accuracy test:  0.9623318385650225\n",
            "Epoch  7  Loss training :  2.116482734680176  Accuracy test:  0.9605381165919282\n",
            "Epoch  8  Loss training :  1.8632936477661133  Accuracy test:  0.9542600896860987\n",
            "Epoch  9  Loss training :  1.6663497686386108  Accuracy test:  0.9614349775784753\n",
            "average accuracy model #3: 0.9614349775784753\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 4\n",
            "Epoch  0  Loss training :  15.384475708007812  Accuracy test:  0.9587443946188341\n",
            "Epoch  1  Loss training :  5.657888889312744  Accuracy test:  0.9587443946188341\n",
            "Epoch  2  Loss training :  4.283143520355225  Accuracy test:  0.9488789237668162\n",
            "Epoch  3  Loss training :  3.8136277198791504  Accuracy test:  0.9488789237668162\n",
            "Epoch  4  Loss training :  2.7791695594787598  Accuracy test:  0.9542600896860987\n",
            "Epoch  5  Loss training :  58.50846481323242  Accuracy test:  0.9542600896860987\n",
            "Epoch  6  Loss training :  1.6109920740127563  Accuracy test:  0.9515695067264573\n",
            "Epoch  7  Loss training :  1.4675531387329102  Accuracy test:  0.9488789237668162\n",
            "Epoch  8  Loss training :  1.2856465578079224  Accuracy test:  0.9551569506726457\n",
            "Epoch  9  Loss training :  1.2232816219329834  Accuracy test:  0.9605381165919282\n",
            "average accuracy model #4: 0.9605381165919282\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 5\n",
            "Epoch  0  Loss training :  16.19429588317871  Accuracy test:  0.9488789237668162\n",
            "Epoch  1  Loss training :  6.134147644042969  Accuracy test:  0.9417040358744395\n",
            "Epoch  2  Loss training :  4.557230472564697  Accuracy test:  0.9587443946188341\n",
            "Epoch  3  Loss training :  3.2602555751800537  Accuracy test:  0.9560538116591928\n",
            "Epoch  4  Loss training :  2.380026340484619  Accuracy test:  0.9587443946188341\n",
            "Epoch  5  Loss training :  2.029839038848877  Accuracy test:  0.9614349775784753\n",
            "Epoch  6  Loss training :  1.7056077718734741  Accuracy test:  0.9623318385650225\n",
            "Epoch  7  Loss training :  1.5521962642669678  Accuracy test:  0.9605381165919282\n",
            "Epoch  8  Loss training :  1.2818493843078613  Accuracy test:  0.95695067264574\n",
            "Epoch  9  Loss training :  1.280906319618225  Accuracy test:  0.9524663677130045\n",
            "average accuracy model #5: 0.9524663677130045\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 6\n",
            "Epoch  0  Loss training :  17.50907325744629  Accuracy test:  0.9659192825112107\n",
            "Epoch  1  Loss training :  5.602392196655273  Accuracy test:  0.9614349775784753\n",
            "Epoch  2  Loss training :  4.015988826751709  Accuracy test:  0.9623318385650225\n",
            "Epoch  3  Loss training :  3.1795942783355713  Accuracy test:  0.9605381165919282\n",
            "Epoch  4  Loss training :  2.370129346847534  Accuracy test:  0.957847533632287\n",
            "Epoch  5  Loss training :  1.9966886043548584  Accuracy test:  0.9605381165919282\n",
            "Epoch  6  Loss training :  4.124104022979736  Accuracy test:  0.9641255605381166\n",
            "Epoch  7  Loss training :  1.76272451877594  Accuracy test:  0.9614349775784753\n",
            "Epoch  8  Loss training :  1.5800784826278687  Accuracy test:  0.9587443946188341\n",
            "Epoch  9  Loss training :  1.609055757522583  Accuracy test:  0.9641255605381166\n",
            "average accuracy model #6: 0.9641255605381166\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 7\n",
            "Epoch  0  Loss training :  13.015170097351074  Accuracy test:  0.9264573991031391\n",
            "Epoch  1  Loss training :  5.5925469398498535  Accuracy test:  0.9443946188340807\n",
            "Epoch  2  Loss training :  4.074308395385742  Accuracy test:  0.9309417040358744\n",
            "Epoch  3  Loss training :  3.497072696685791  Accuracy test:  0.9300448430493273\n",
            "Epoch  4  Loss training :  2.7898812294006348  Accuracy test:  0.9434977578475336\n",
            "Epoch  5  Loss training :  2.167186737060547  Accuracy test:  0.9354260089686098\n",
            "Epoch  6  Loss training :  1.9887949228286743  Accuracy test:  0.9354260089686098\n",
            "Epoch  7  Loss training :  15.324005126953125  Accuracy test:  0.9443946188340807\n",
            "Epoch  8  Loss training :  2.891692638397217  Accuracy test:  0.9461883408071748\n",
            "Epoch  9  Loss training :  2.061131000518799  Accuracy test:  0.9452914798206278\n",
            "average accuracy model #7: 0.9452914798206278\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 8\n",
            "Epoch  0  Loss training :  15.901036262512207  Accuracy test:  0.9318385650224216\n",
            "Epoch  1  Loss training :  5.380066871643066  Accuracy test:  0.9408071748878923\n",
            "Epoch  2  Loss training :  4.119558811187744  Accuracy test:  0.9479820627802691\n",
            "Epoch  3  Loss training :  3.349191904067993  Accuracy test:  0.9318385650224216\n",
            "Epoch  4  Loss training :  2.952996253967285  Accuracy test:  0.9426008968609866\n",
            "Epoch  5  Loss training :  2.3401331901550293  Accuracy test:  0.9479820627802691\n",
            "Epoch  6  Loss training :  1.8206048011779785  Accuracy test:  0.9524663677130045\n",
            "Epoch  7  Loss training :  1.5174534320831299  Accuracy test:  0.9013452914798207\n",
            "Epoch  8  Loss training :  1.9166280031204224  Accuracy test:  0.9551569506726457\n",
            "Epoch  9  Loss training :  1.2489253282546997  Accuracy test:  0.9452914798206278\n",
            "average accuracy model #8: 0.9452914798206278\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 9\n",
            "Epoch  0  Loss training :  14.367324829101562  Accuracy test:  0.9551569506726457\n",
            "Epoch  1  Loss training :  5.820113658905029  Accuracy test:  0.9461883408071748\n",
            "Epoch  2  Loss training :  4.344776153564453  Accuracy test:  0.957847533632287\n",
            "Epoch  3  Loss training :  3.4965577125549316  Accuracy test:  0.9327354260089686\n",
            "Epoch  4  Loss training :  2.833460807800293  Accuracy test:  0.9300448430493273\n",
            "Epoch  5  Loss training :  1.8306012153625488  Accuracy test:  0.9515695067264573\n",
            "Epoch  6  Loss training :  1.9740159511566162  Accuracy test:  0.9434977578475336\n",
            "Epoch  7  Loss training :  1.7332825660705566  Accuracy test:  0.9551569506726457\n",
            "Epoch  8  Loss training :  1.4038432836532593  Accuracy test:  0.9542600896860987\n",
            "Epoch  9  Loss training :  1.3352530002593994  Accuracy test:  0.9587443946188341\n",
            "average accuracy model #9: 0.9587443946188341\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hyper parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # 50\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Model #\", i)\n",
        "  model = LSTM_Classifier(hidden_dim=20)  # Dimension = 20\n",
        "  # Train preprocessor 2\n",
        "  train_model(model, loader_training_v2, loader_test_v2, epochs, learning_rate)\n",
        "  accuracies = evaluate_model(model, loader_test_v2)\n",
        "  print(f\"average accuracy model #{i}: {accuracies.mean()}\")\n",
        "  print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLu1X4xCz7PQ"
      },
      "source": [
        "Test: D = 100, preprocess version 2, 10 models test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYZekC2mobFj",
        "outputId": "2569f7f6-2514-4348-dd27-257701396bbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model # 0\n",
            "Epoch  0  Loss training :  40.62193298339844  Accuracy test:  0.9713004484304932\n",
            "Epoch  1  Loss training :  6.095354080200195  Accuracy test:  0.9515695067264573\n",
            "Epoch  2  Loss training :  4.000911712646484  Accuracy test:  0.9632286995515695\n",
            "Epoch  3  Loss training :  3.010891914367676  Accuracy test:  0.9560538116591928\n",
            "Epoch  4  Loss training :  2.389524221420288  Accuracy test:  0.947085201793722\n",
            "Epoch  5  Loss training :  7.597579479217529  Accuracy test:  0.9327354260089686\n",
            "Epoch  6  Loss training :  2.5797417163848877  Accuracy test:  0.9641255605381166\n",
            "Epoch  7  Loss training :  1.0570229291915894  Accuracy test:  0.947085201793722\n",
            "Epoch  8  Loss training :  1.48758065700531  Accuracy test:  0.9524663677130045\n",
            "Epoch  9  Loss training :  1.0288139581680298  Accuracy test:  0.9506726457399103\n",
            "average accuracy model #0: 0.9506726457399103\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 1\n",
            "Epoch  0  Loss training :  53.991249084472656  Accuracy test:  0.9668161434977578\n",
            "Epoch  1  Loss training :  9.018210411071777  Accuracy test:  0.9551569506726457\n",
            "Epoch  2  Loss training :  6.183427810668945  Accuracy test:  0.9506726457399103\n",
            "Epoch  3  Loss training :  19.36691665649414  Accuracy test:  0.9246636771300448\n",
            "Epoch  4  Loss training :  9.648547172546387  Accuracy test:  0.9506726457399103\n",
            "Epoch  5  Loss training :  5.673180103302002  Accuracy test:  0.9506726457399103\n",
            "Epoch  6  Loss training :  4.319581508636475  Accuracy test:  0.9623318385650225\n",
            "Epoch  7  Loss training :  3.016803503036499  Accuracy test:  0.9524663677130045\n",
            "Epoch  8  Loss training :  2.3846163749694824  Accuracy test:  0.967713004484305\n",
            "Epoch  9  Loss training :  1.9318419694900513  Accuracy test:  0.9524663677130045\n",
            "average accuracy model #1: 0.9524663677130045\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 2\n",
            "Epoch  0  Loss training :  48.86873245239258  Accuracy test:  0.968609865470852\n",
            "Epoch  1  Loss training :  8.422236442565918  Accuracy test:  0.9524663677130045\n",
            "Epoch  2  Loss training :  6.0494561195373535  Accuracy test:  0.9488789237668162\n",
            "Epoch  3  Loss training :  4.314355850219727  Accuracy test:  0.9497757847533632\n",
            "Epoch  4  Loss training :  3.5226898193359375  Accuracy test:  0.9587443946188341\n",
            "Epoch  5  Loss training :  2.8816826343536377  Accuracy test:  0.9479820627802691\n",
            "Epoch  6  Loss training :  11.499330520629883  Accuracy test:  0.8582959641255605\n",
            "Epoch  7  Loss training :  4.780432224273682  Accuracy test:  0.9461883408071748\n",
            "Epoch  8  Loss training :  2.589843273162842  Accuracy test:  0.9605381165919282\n",
            "Epoch  9  Loss training :  1.781441569328308  Accuracy test:  0.9614349775784753\n",
            "average accuracy model #2: 0.9587443946188341\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 3\n",
            "Epoch  0  Loss training :  99.18891143798828  Accuracy test:  0.9605381165919282\n",
            "Epoch  1  Loss training :  6.242822647094727  Accuracy test:  0.9336322869955157\n",
            "Epoch  2  Loss training :  4.145315647125244  Accuracy test:  0.9210762331838565\n",
            "Epoch  3  Loss training :  3.6891109943389893  Accuracy test:  0.9363228699551569\n",
            "Epoch  4  Loss training :  2.333242893218994  Accuracy test:  0.9426008968609866\n",
            "Epoch  5  Loss training :  1.6813890933990479  Accuracy test:  0.9219730941704036\n",
            "Epoch  6  Loss training :  2.119417190551758  Accuracy test:  0.947085201793722\n",
            "Epoch  7  Loss training :  3.0481197834014893  Accuracy test:  0.9623318385650225\n",
            "Epoch  8  Loss training :  1.818292260169983  Accuracy test:  0.9497757847533632\n",
            "Epoch  9  Loss training :  1.0336509943008423  Accuracy test:  0.9560538116591928\n",
            "average accuracy model #3: 0.9560538116591928\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 4\n",
            "Epoch  0  Loss training :  447.8659362792969  Accuracy test:  0.8681614349775785\n",
            "Epoch  1  Loss training :  15.905595779418945  Accuracy test:  0.9587443946188341\n",
            "Epoch  2  Loss training :  6.703669548034668  Accuracy test:  0.9524663677130045\n",
            "Epoch  3  Loss training :  5.096975803375244  Accuracy test:  0.9452914798206278\n",
            "Epoch  4  Loss training :  4.161042213439941  Accuracy test:  0.9524663677130045\n",
            "Epoch  5  Loss training :  4.089095592498779  Accuracy test:  0.9327354260089686\n",
            "Epoch  6  Loss training :  3.0610525608062744  Accuracy test:  0.947085201793722\n",
            "Epoch  7  Loss training :  5.001718044281006  Accuracy test:  0.9506726457399103\n",
            "Epoch  8  Loss training :  3.16428279876709  Accuracy test:  0.95695067264574\n",
            "Epoch  9  Loss training :  2.905848503112793  Accuracy test:  0.9533632286995516\n",
            "average accuracy model #4: 0.9533632286995516\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 5\n",
            "Epoch  0  Loss training :  297.73480224609375  Accuracy test:  0.9443946188340807\n",
            "Epoch  1  Loss training :  8.496636390686035  Accuracy test:  0.9560538116591928\n",
            "Epoch  2  Loss training :  4.656229496002197  Accuracy test:  0.9273542600896861\n",
            "Epoch  3  Loss training :  3.619701385498047  Accuracy test:  0.9237668161434978\n",
            "Epoch  4  Loss training :  2.7000434398651123  Accuracy test:  0.9488789237668162\n",
            "Epoch  5  Loss training :  6.490561008453369  Accuracy test:  0.9650224215246637\n",
            "Epoch  6  Loss training :  2.8368852138519287  Accuracy test:  0.9587443946188341\n",
            "Epoch  7  Loss training :  1.9626766443252563  Accuracy test:  0.9695067264573991\n",
            "Epoch  8  Loss training :  1.6443092823028564  Accuracy test:  0.9542600896860987\n",
            "Epoch  9  Loss training :  0.9708206653594971  Accuracy test:  0.9668161434977578\n",
            "average accuracy model #5: 0.9668161434977578\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 6\n",
            "Epoch  0  Loss training :  31.11309814453125  Accuracy test:  0.8959641255605382\n",
            "Epoch  1  Loss training :  5.743222236633301  Accuracy test:  0.9273542600896861\n",
            "Epoch  2  Loss training :  3.7445149421691895  Accuracy test:  0.9542600896860987\n",
            "Epoch  3  Loss training :  2.762749195098877  Accuracy test:  0.947085201793722\n",
            "Epoch  4  Loss training :  2.3886847496032715  Accuracy test:  0.9336322869955157\n",
            "Epoch  5  Loss training :  3.092484951019287  Accuracy test:  0.9641255605381166\n",
            "Epoch  6  Loss training :  2.5656962394714355  Accuracy test:  0.9587443946188341\n",
            "Epoch  7  Loss training :  1.307411551475525  Accuracy test:  0.9426008968609866\n",
            "Epoch  8  Loss training :  1.3188766241073608  Accuracy test:  0.9381165919282511\n",
            "Epoch  9  Loss training :  1.5762304067611694  Accuracy test:  0.9399103139013453\n",
            "average accuracy model #6: 0.9408071748878923\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 7\n",
            "Epoch  0  Loss training :  35.89949035644531  Accuracy test:  0.9641255605381166\n",
            "Epoch  1  Loss training :  5.990467071533203  Accuracy test:  0.9713004484304932\n",
            "Epoch  2  Loss training :  3.836698293685913  Accuracy test:  0.9659192825112107\n",
            "Epoch  3  Loss training :  3.258598804473877  Accuracy test:  0.9506726457399103\n",
            "Epoch  4  Loss training :  2.171935796737671  Accuracy test:  0.9695067264573991\n",
            "Epoch  5  Loss training :  2.294088363647461  Accuracy test:  0.9641255605381166\n",
            "Epoch  6  Loss training :  2.5540647506713867  Accuracy test:  0.9560538116591928\n",
            "Epoch  7  Loss training :  3.6744003295898438  Accuracy test:  0.9713004484304932\n",
            "Epoch  8  Loss training :  1.804265022277832  Accuracy test:  0.9695067264573991\n",
            "Epoch  9  Loss training :  1.0003941059112549  Accuracy test:  0.9641255605381166\n",
            "average accuracy model #7: 0.967713004484305\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 8\n",
            "Epoch  0  Loss training :  158.2152099609375  Accuracy test:  0.9551569506726457\n",
            "Epoch  1  Loss training :  8.54315185546875  Accuracy test:  0.9623318385650225\n",
            "Epoch  2  Loss training :  5.7928996086120605  Accuracy test:  0.9659192825112107\n",
            "Epoch  3  Loss training :  4.533592700958252  Accuracy test:  0.9461883408071748\n",
            "Epoch  4  Loss training :  3.5876147747039795  Accuracy test:  0.9524663677130045\n",
            "Epoch  5  Loss training :  3.026104211807251  Accuracy test:  0.9587443946188341\n",
            "Epoch  6  Loss training :  2.988447904586792  Accuracy test:  0.9560538116591928\n",
            "Epoch  7  Loss training :  2.409513473510742  Accuracy test:  0.9426008968609866\n",
            "Epoch  8  Loss training :  1.610999345779419  Accuracy test:  0.9515695067264573\n",
            "Epoch  9  Loss training :  1.3765335083007812  Accuracy test:  0.9641255605381166\n",
            "average accuracy model #8: 0.9641255605381166\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 9\n",
            "Epoch  0  Loss training :  124.22151184082031  Accuracy test:  0.885201793721973\n",
            "Epoch  1  Loss training :  10.520835876464844  Accuracy test:  0.9596412556053812\n",
            "Epoch  2  Loss training :  6.8965163230896  Accuracy test:  0.9650224215246637\n",
            "Epoch  3  Loss training :  6.031528472900391  Accuracy test:  0.9632286995515695\n",
            "Epoch  4  Loss training :  4.763447284698486  Accuracy test:  0.9668161434977578\n",
            "Epoch  5  Loss training :  3.832256555557251  Accuracy test:  0.95695067264574\n",
            "Epoch  6  Loss training :  3.472736358642578  Accuracy test:  0.9641255605381166\n",
            "Epoch  7  Loss training :  2.0429654121398926  Accuracy test:  0.9605381165919282\n",
            "Epoch  8  Loss training :  4.498635292053223  Accuracy test:  0.9596412556053812\n",
            "Epoch  9  Loss training :  2.2378432750701904  Accuracy test:  0.967713004484305\n",
            "average accuracy model #9: 0.967713004484305\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hyper parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # 50\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Model #\", i)\n",
        "  model_100 = LSTM_Classifier(hidden_dim=100)  # Dimension = 100\n",
        "  # Train preprocessor 2\n",
        "  train_model(model_100, loader_training_v2, loader_test_v2, epochs, learning_rate)\n",
        "  accuracies = evaluate_model(model_100, loader_test_v2)\n",
        "  print(f\"average accuracy model #{i}: {accuracies.mean()}\")\n",
        "  print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AVXbfn3z_YU"
      },
      "source": [
        "Test: D = 20, preprocess version 1, 10 models test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVDRfGRX0FJ6",
        "outputId": "adc967ff-e32b-4bc6-d596-76067193bf3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model # 0\n",
            "Epoch  0  Loss training :  14.535436630249023  Accuracy test:  0.9417040358744395\n",
            "Epoch  1  Loss training :  5.504261016845703  Accuracy test:  0.9390134529147982\n",
            "Epoch  2  Loss training :  4.109217643737793  Accuracy test:  0.9479820627802691\n",
            "Epoch  3  Loss training :  3.0082881450653076  Accuracy test:  0.9461883408071748\n",
            "Epoch  4  Loss training :  2.3335564136505127  Accuracy test:  0.9345291479820628\n",
            "Epoch  5  Loss training :  2.002533435821533  Accuracy test:  0.9327354260089686\n",
            "Epoch  6  Loss training :  1.5289751291275024  Accuracy test:  0.9560538116591928\n",
            "Epoch  7  Loss training :  4.377479553222656  Accuracy test:  0.9399103139013453\n",
            "Epoch  8  Loss training :  2.274275302886963  Accuracy test:  0.9417040358744395\n",
            "Epoch  9  Loss training :  1.2916669845581055  Accuracy test:  0.9479820627802691\n",
            "average accuracy model #0: 0.9488789237668162\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 1\n",
            "Epoch  0  Loss training :  14.743879318237305  Accuracy test:  0.9452914798206278\n",
            "Epoch  1  Loss training :  5.867557048797607  Accuracy test:  0.9596412556053812\n",
            "Epoch  2  Loss training :  4.756059169769287  Accuracy test:  0.9282511210762332\n",
            "Epoch  3  Loss training :  2.9763548374176025  Accuracy test:  0.95695067264574\n",
            "Epoch  4  Loss training :  2.596059799194336  Accuracy test:  0.957847533632287\n",
            "Epoch  5  Loss training :  2.068911552429199  Accuracy test:  0.9533632286995516\n",
            "Epoch  6  Loss training :  1.877404808998108  Accuracy test:  0.95695067264574\n",
            "Epoch  7  Loss training :  1.3958524465560913  Accuracy test:  0.95695067264574\n",
            "Epoch  8  Loss training :  1.290757417678833  Accuracy test:  0.9587443946188341\n",
            "Epoch  9  Loss training :  1.985700011253357  Accuracy test:  0.9605381165919282\n",
            "average accuracy model #1: 0.9605381165919282\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 2\n",
            "Epoch  0  Loss training :  16.13970375061035  Accuracy test:  0.9479820627802691\n",
            "Epoch  1  Loss training :  6.036886215209961  Accuracy test:  0.9264573991031391\n",
            "Epoch  2  Loss training :  4.428367614746094  Accuracy test:  0.9372197309417041\n",
            "Epoch  3  Loss training :  4.735888481140137  Accuracy test:  0.9183856502242153\n",
            "Epoch  4  Loss training :  3.6136250495910645  Accuracy test:  0.947085201793722\n",
            "Epoch  5  Loss training :  2.908550500869751  Accuracy test:  0.9417040358744395\n",
            "Epoch  6  Loss training :  2.6254220008850098  Accuracy test:  0.9300448430493273\n",
            "Epoch  7  Loss training :  2.70032000541687  Accuracy test:  0.8654708520179372\n",
            "Epoch  8  Loss training :  2.233499050140381  Accuracy test:  0.8941704035874439\n",
            "Epoch  9  Loss training :  1.594187617301941  Accuracy test:  0.9246636771300448\n",
            "average accuracy model #2: 0.915695067264574\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 3\n",
            "Epoch  0  Loss training :  17.097299575805664  Accuracy test:  0.9650224215246637\n",
            "Epoch  1  Loss training :  5.98389196395874  Accuracy test:  0.9596412556053812\n",
            "Epoch  2  Loss training :  3.999415636062622  Accuracy test:  0.9632286995515695\n",
            "Epoch  3  Loss training :  3.066589117050171  Accuracy test:  0.9524663677130045\n",
            "Epoch  4  Loss training :  2.8478972911834717  Accuracy test:  0.9488789237668162\n",
            "Epoch  5  Loss training :  1.8900636434555054  Accuracy test:  0.9596412556053812\n",
            "Epoch  6  Loss training :  1.8136087656021118  Accuracy test:  0.9659192825112107\n",
            "Epoch  7  Loss training :  1.3174890279769897  Accuracy test:  0.9614349775784753\n",
            "Epoch  8  Loss training :  1.2816684246063232  Accuracy test:  0.9650224215246637\n",
            "Epoch  9  Loss training :  2.7138655185699463  Accuracy test:  0.9623318385650225\n",
            "average accuracy model #3: 0.9623318385650225\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 4\n",
            "Epoch  0  Loss training :  13.878317832946777  Accuracy test:  0.9650224215246637\n",
            "Epoch  1  Loss training :  5.332579135894775  Accuracy test:  0.9013452914798207\n",
            "Epoch  2  Loss training :  4.066873550415039  Accuracy test:  0.9174887892376682\n",
            "Epoch  3  Loss training :  3.0566606521606445  Accuracy test:  0.9201793721973094\n",
            "Epoch  4  Loss training :  2.423616409301758  Accuracy test:  0.9174887892376682\n",
            "Epoch  5  Loss training :  1.8056122064590454  Accuracy test:  0.9318385650224216\n",
            "Epoch  6  Loss training :  1.4292031526565552  Accuracy test:  0.9488789237668162\n",
            "Epoch  7  Loss training :  1.411295771598816  Accuracy test:  0.9434977578475336\n",
            "Epoch  8  Loss training :  3.066134452819824  Accuracy test:  0.9336322869955157\n",
            "Epoch  9  Loss training :  1.6042615175247192  Accuracy test:  0.957847533632287\n",
            "average accuracy model #4: 0.957847533632287\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 5\n",
            "Epoch  0  Loss training :  14.071977615356445  Accuracy test:  0.9560538116591928\n",
            "Epoch  1  Loss training :  5.406245708465576  Accuracy test:  0.9560538116591928\n",
            "Epoch  2  Loss training :  4.263199806213379  Accuracy test:  0.9497757847533632\n",
            "Epoch  3  Loss training :  3.0985043048858643  Accuracy test:  0.9533632286995516\n",
            "Epoch  4  Loss training :  2.2683815956115723  Accuracy test:  0.9605381165919282\n",
            "Epoch  5  Loss training :  2.472642660140991  Accuracy test:  0.95695067264574\n",
            "Epoch  6  Loss training :  1.8129669427871704  Accuracy test:  0.9372197309417041\n",
            "Epoch  7  Loss training :  1.4726688861846924  Accuracy test:  0.9506726457399103\n",
            "Epoch  8  Loss training :  1.3877856731414795  Accuracy test:  0.9336322869955157\n",
            "Epoch  9  Loss training :  1.3292415142059326  Accuracy test:  0.9488789237668162\n",
            "average accuracy model #5: 0.9488789237668162\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 6\n",
            "Epoch  0  Loss training :  18.682594299316406  Accuracy test:  0.95695067264574\n",
            "Epoch  1  Loss training :  6.38621711730957  Accuracy test:  0.9390134529147982\n",
            "Epoch  2  Loss training :  5.1881561279296875  Accuracy test:  0.9479820627802691\n",
            "Epoch  3  Loss training :  4.174950122833252  Accuracy test:  0.9497757847533632\n",
            "Epoch  4  Loss training :  3.4700350761413574  Accuracy test:  0.9506726457399103\n",
            "Epoch  5  Loss training :  3.53898549079895  Accuracy test:  0.9650224215246637\n",
            "Epoch  6  Loss training :  2.5183157920837402  Accuracy test:  0.9596412556053812\n",
            "Epoch  7  Loss training :  2.6930484771728516  Accuracy test:  0.9596412556053812\n",
            "Epoch  8  Loss training :  1.7787269353866577  Accuracy test:  0.9641255605381166\n",
            "Epoch  9  Loss training :  1.9569594860076904  Accuracy test:  0.9623318385650225\n",
            "average accuracy model #6: 0.9623318385650225\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 7\n",
            "Epoch  0  Loss training :  16.048046112060547  Accuracy test:  0.9183856502242153\n",
            "Epoch  1  Loss training :  6.342408180236816  Accuracy test:  0.957847533632287\n",
            "Epoch  2  Loss training :  5.152667045593262  Accuracy test:  0.9497757847533632\n",
            "Epoch  3  Loss training :  3.7762291431427  Accuracy test:  0.9479820627802691\n",
            "Epoch  4  Loss training :  3.0539095401763916  Accuracy test:  0.9309417040358744\n",
            "Epoch  5  Loss training :  2.2698822021484375  Accuracy test:  0.8932735426008969\n",
            "Epoch  6  Loss training :  2.581780433654785  Accuracy test:  0.9336322869955157\n",
            "Epoch  7  Loss training :  1.758584976196289  Accuracy test:  0.9399103139013453\n",
            "Epoch  8  Loss training :  1.270507574081421  Accuracy test:  0.9506726457399103\n",
            "Epoch  9  Loss training :  1.1149533987045288  Accuracy test:  0.9479820627802691\n",
            "average accuracy model #7: 0.9479820627802691\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 8\n",
            "Epoch  0  Loss training :  14.036226272583008  Accuracy test:  0.9417040358744395\n",
            "Epoch  1  Loss training :  5.626523971557617  Accuracy test:  0.9336322869955157\n",
            "Epoch  2  Loss training :  4.118374824523926  Accuracy test:  0.9452914798206278\n",
            "Epoch  3  Loss training :  3.532362461090088  Accuracy test:  0.9524663677130045\n",
            "Epoch  4  Loss training :  2.52681040763855  Accuracy test:  0.9479820627802691\n",
            "Epoch  5  Loss training :  2.071148157119751  Accuracy test:  0.9524663677130045\n",
            "Epoch  6  Loss training :  1.9468345642089844  Accuracy test:  0.9408071748878923\n",
            "Epoch  7  Loss training :  3.2267684936523438  Accuracy test:  0.9390134529147982\n",
            "Epoch  8  Loss training :  1.8588894605636597  Accuracy test:  0.9399103139013453\n",
            "Epoch  9  Loss training :  1.270458459854126  Accuracy test:  0.9479820627802691\n",
            "average accuracy model #8: 0.9479820627802691\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 9\n",
            "Epoch  0  Loss training :  15.758584976196289  Accuracy test:  0.9390134529147982\n",
            "Epoch  1  Loss training :  5.9688849449157715  Accuracy test:  0.9246636771300448\n",
            "Epoch  2  Loss training :  4.334054470062256  Accuracy test:  0.9345291479820628\n",
            "Epoch  3  Loss training :  3.1580848693847656  Accuracy test:  0.9031390134529148\n",
            "Epoch  4  Loss training :  3.0453314781188965  Accuracy test:  0.9381165919282511\n",
            "Epoch  5  Loss training :  2.3311030864715576  Accuracy test:  0.9327354260089686\n",
            "Epoch  6  Loss training :  2.382591724395752  Accuracy test:  0.9399103139013453\n",
            "Epoch  7  Loss training :  1.6533602476119995  Accuracy test:  0.9596412556053812\n",
            "Epoch  8  Loss training :  1.2521655559539795  Accuracy test:  0.9596412556053812\n",
            "Epoch  9  Loss training :  1.2690932750701904  Accuracy test:  0.957847533632287\n",
            "average accuracy model #9: 0.9587443946188341\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hyper parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # 50\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Model #\", i)\n",
        "  model = LSTM_Classifier(hidden_dim=20)  # Dimension = 20\n",
        "  # Train preprocessor 2\n",
        "  train_model(model, loader_training_v1, loader_test_v1, epochs, learning_rate)\n",
        "  accuracies = evaluate_model(model, loader_test_v1)\n",
        "  print(f\"average accuracy model #{i}: {accuracies.mean()}\")\n",
        "  print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGziquTb0BIU"
      },
      "source": [
        "Test: D = 100, preprocess version 1, 10 models test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk9wYWHP0LWF",
        "outputId": "4567467b-107b-482a-97bb-cedce9d6ae18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model # 0\n",
            "Epoch  0  Loss training :  721.6635131835938  Accuracy test:  0.8807174887892377\n",
            "Epoch  1  Loss training :  634.4773559570312  Accuracy test:  0.8807174887892377\n",
            "Epoch  2  Loss training :  12.37992000579834  Accuracy test:  0.9668161434977578\n",
            "Epoch  3  Loss training :  5.625229835510254  Accuracy test:  0.9668161434977578\n",
            "Epoch  4  Loss training :  3.593289852142334  Accuracy test:  0.9605381165919282\n",
            "Epoch  5  Loss training :  2.9294116497039795  Accuracy test:  0.9650224215246637\n",
            "Epoch  6  Loss training :  2.4447474479675293  Accuracy test:  0.947085201793722\n",
            "Epoch  7  Loss training :  2.2420754432678223  Accuracy test:  0.9668161434977578\n",
            "Epoch  8  Loss training :  17.86869239807129  Accuracy test:  0.9461883408071748\n",
            "Epoch  9  Loss training :  7.250354766845703  Accuracy test:  0.9668161434977578\n",
            "average accuracy model #0: 0.9650224215246637\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 1\n",
            "Epoch  0  Loss training :  49.65599060058594  Accuracy test:  0.9022421524663677\n",
            "Epoch  1  Loss training :  12.072321891784668  Accuracy test:  0.9219730941704036\n",
            "Epoch  2  Loss training :  9.249421119689941  Accuracy test:  0.9551569506726457\n",
            "Epoch  3  Loss training :  5.647680759429932  Accuracy test:  0.9408071748878923\n",
            "Epoch  4  Loss training :  4.619322299957275  Accuracy test:  0.95695067264574\n",
            "Epoch  5  Loss training :  3.7910830974578857  Accuracy test:  0.9587443946188341\n",
            "Epoch  6  Loss training :  3.844966411590576  Accuracy test:  0.957847533632287\n",
            "Epoch  7  Loss training :  3.4401445388793945  Accuracy test:  0.9605381165919282\n",
            "Epoch  8  Loss training :  3.6989786624908447  Accuracy test:  0.9704035874439462\n",
            "Epoch  9  Loss training :  2.6224277019500732  Accuracy test:  0.9739910313901345\n",
            "average accuracy model #1: 0.9739910313901345\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 2\n",
            "Epoch  0  Loss training :  33.770816802978516  Accuracy test:  0.9327354260089686\n",
            "Epoch  1  Loss training :  7.3915934562683105  Accuracy test:  0.9614349775784753\n",
            "Epoch  2  Loss training :  5.098598957061768  Accuracy test:  0.9668161434977578\n",
            "Epoch  3  Loss training :  4.362366199493408  Accuracy test:  0.9695067264573991\n",
            "Epoch  4  Loss training :  3.1024322509765625  Accuracy test:  0.9668161434977578\n",
            "Epoch  5  Loss training :  2.3946220874786377  Accuracy test:  0.9650224215246637\n",
            "Epoch  6  Loss training :  2.347698926925659  Accuracy test:  0.9748878923766816\n",
            "Epoch  7  Loss training :  3.0227608680725098  Accuracy test:  0.9713004484304932\n",
            "Epoch  8  Loss training :  2.681549310684204  Accuracy test:  0.967713004484305\n",
            "Epoch  9  Loss training :  1.8403964042663574  Accuracy test:  0.9668161434977578\n",
            "average accuracy model #2: 0.9668161434977578\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 3\n",
            "Epoch  0  Loss training :  142.93771362304688  Accuracy test:  0.9112107623318386\n",
            "Epoch  1  Loss training :  8.379878997802734  Accuracy test:  0.9596412556053812\n",
            "Epoch  2  Loss training :  5.821214199066162  Accuracy test:  0.9452914798206278\n",
            "Epoch  3  Loss training :  6.10660457611084  Accuracy test:  0.9426008968609866\n",
            "Epoch  4  Loss training :  4.483349800109863  Accuracy test:  0.9596412556053812\n",
            "Epoch  5  Loss training :  3.6043360233306885  Accuracy test:  0.9623318385650225\n",
            "Epoch  6  Loss training :  3.181561231613159  Accuracy test:  0.95695067264574\n",
            "Epoch  7  Loss training :  2.299687385559082  Accuracy test:  0.9587443946188341\n",
            "Epoch  8  Loss training :  1.8206028938293457  Accuracy test:  0.9650224215246637\n",
            "Epoch  9  Loss training :  1.7413302659988403  Accuracy test:  0.9560538116591928\n",
            "average accuracy model #3: 0.9560538116591928\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 4\n",
            "Epoch  0  Loss training :  35.79410934448242  Accuracy test:  0.9560538116591928\n",
            "Epoch  1  Loss training :  7.974372863769531  Accuracy test:  0.9641255605381166\n",
            "Epoch  2  Loss training :  5.171741485595703  Accuracy test:  0.9533632286995516\n",
            "Epoch  3  Loss training :  4.754111289978027  Accuracy test:  0.9659192825112107\n",
            "Epoch  4  Loss training :  3.3045220375061035  Accuracy test:  0.9605381165919282\n",
            "Epoch  5  Loss training :  3.0964787006378174  Accuracy test:  0.9659192825112107\n",
            "Epoch  6  Loss training :  3.09381103515625  Accuracy test:  0.9713004484304932\n",
            "Epoch  7  Loss training :  4.566738128662109  Accuracy test:  0.9659192825112107\n",
            "Epoch  8  Loss training :  2.6991705894470215  Accuracy test:  0.9560538116591928\n",
            "Epoch  9  Loss training :  1.7643986940383911  Accuracy test:  0.9605381165919282\n",
            "average accuracy model #4: 0.9605381165919282\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 5\n",
            "Epoch  0  Loss training :  47.807518005371094  Accuracy test:  0.9614349775784753\n",
            "Epoch  1  Loss training :  6.633252143859863  Accuracy test:  0.9587443946188341\n",
            "Epoch  2  Loss training :  5.721080780029297  Accuracy test:  0.9596412556053812\n",
            "Epoch  3  Loss training :  3.7982640266418457  Accuracy test:  0.9659192825112107\n",
            "Epoch  4  Loss training :  3.5199289321899414  Accuracy test:  0.9659192825112107\n",
            "Epoch  5  Loss training :  3.722384214401245  Accuracy test:  0.9524663677130045\n",
            "Epoch  6  Loss training :  2.9287872314453125  Accuracy test:  0.9668161434977578\n",
            "Epoch  7  Loss training :  2.0920445919036865  Accuracy test:  0.9632286995515695\n",
            "Epoch  8  Loss training :  1.8578269481658936  Accuracy test:  0.967713004484305\n",
            "Epoch  9  Loss training :  2.1785888671875  Accuracy test:  0.9605381165919282\n",
            "average accuracy model #5: 0.9605381165919282\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 6\n",
            "Epoch  0  Loss training :  52.58697509765625  Accuracy test:  0.9497757847533632\n",
            "Epoch  1  Loss training :  9.082552909851074  Accuracy test:  0.957847533632287\n",
            "Epoch  2  Loss training :  6.820034027099609  Accuracy test:  0.95695067264574\n",
            "Epoch  3  Loss training :  4.607119560241699  Accuracy test:  0.9497757847533632\n",
            "Epoch  4  Loss training :  4.031620502471924  Accuracy test:  0.9488789237668162\n",
            "Epoch  5  Loss training :  3.5451016426086426  Accuracy test:  0.9560538116591928\n",
            "Epoch  6  Loss training :  3.8114733695983887  Accuracy test:  0.95695067264574\n",
            "Epoch  7  Loss training :  3.5111184120178223  Accuracy test:  0.9452914798206278\n",
            "Epoch  8  Loss training :  2.440520763397217  Accuracy test:  0.9650224215246637\n",
            "Epoch  9  Loss training :  3.00642466545105  Accuracy test:  0.9641255605381166\n",
            "average accuracy model #6: 0.9641255605381166\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 7\n",
            "Epoch  0  Loss training :  98.35966491699219  Accuracy test:  0.9426008968609866\n",
            "Epoch  1  Loss training :  11.399496078491211  Accuracy test:  0.9695067264573991\n",
            "Epoch  2  Loss training :  6.929342746734619  Accuracy test:  0.9721973094170404\n",
            "Epoch  3  Loss training :  5.812469482421875  Accuracy test:  0.9596412556053812\n",
            "Epoch  4  Loss training :  4.873218059539795  Accuracy test:  0.9614349775784753\n",
            "Epoch  5  Loss training :  3.800614833831787  Accuracy test:  0.9623318385650225\n",
            "Epoch  6  Loss training :  3.3293142318725586  Accuracy test:  0.9659192825112107\n",
            "Epoch  7  Loss training :  2.983158588409424  Accuracy test:  0.9533632286995516\n",
            "Epoch  8  Loss training :  2.173888921737671  Accuracy test:  0.9623318385650225\n",
            "Epoch  9  Loss training :  2.8744914531707764  Accuracy test:  0.9605381165919282\n",
            "average accuracy model #7: 0.9605381165919282\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 8\n",
            "Epoch  0  Loss training :  87.68462371826172  Accuracy test:  0.967713004484305\n",
            "Epoch  1  Loss training :  8.326164245605469  Accuracy test:  0.9650224215246637\n",
            "Epoch  2  Loss training :  5.230950832366943  Accuracy test:  0.9766816143497757\n",
            "Epoch  3  Loss training :  4.117242813110352  Accuracy test:  0.9695067264573991\n",
            "Epoch  4  Loss training :  3.95751953125  Accuracy test:  0.9551569506726457\n",
            "Epoch  5  Loss training :  3.8262274265289307  Accuracy test:  0.9659192825112107\n",
            "Epoch  6  Loss training :  2.382282257080078  Accuracy test:  0.9650224215246637\n",
            "Epoch  7  Loss training :  2.803257942199707  Accuracy test:  0.9775784753363229\n",
            "Epoch  8  Loss training :  2.4332516193389893  Accuracy test:  0.9730941704035875\n",
            "Epoch  9  Loss training :  2.150393009185791  Accuracy test:  0.9668161434977578\n",
            "average accuracy model #8: 0.9668161434977578\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Model # 9\n",
            "Epoch  0  Loss training :  54.79755401611328  Accuracy test:  0.9641255605381166\n",
            "Epoch  1  Loss training :  7.247409343719482  Accuracy test:  0.9650224215246637\n",
            "Epoch  2  Loss training :  4.83514404296875  Accuracy test:  0.9704035874439462\n",
            "Epoch  3  Loss training :  4.140172481536865  Accuracy test:  0.9614349775784753\n",
            "Epoch  4  Loss training :  3.3362679481506348  Accuracy test:  0.9668161434977578\n",
            "Epoch  5  Loss training :  2.651156187057495  Accuracy test:  0.9659192825112107\n",
            "Epoch  6  Loss training :  2.8162105083465576  Accuracy test:  0.9560538116591928\n",
            "Epoch  7  Loss training :  2.7625467777252197  Accuracy test:  0.9614349775784753\n",
            "Epoch  8  Loss training :  1.9663251638412476  Accuracy test:  0.9479820627802691\n",
            "Epoch  9  Loss training :  1.97831392288208  Accuracy test:  0.9443946188340807\n",
            "average accuracy model #9: 0.9443946188340807\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#hyper parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # 50\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Model #\", i)\n",
        "  model = LSTM_Classifier(hidden_dim=100)  # Dimension = 100\n",
        "  # Train preprocessor 2\n",
        "  train_model(model, loader_training_v1, loader_test_v1, epochs, learning_rate)\n",
        "  accuracies = evaluate_model(model, loader_test_v1)\n",
        "  print(f\"average accuracy model #{i}: {accuracies.mean()}\")\n",
        "  print(\"++++++++++++++++++++++++++++++++++++++++++++++++\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sIyCk65RF7l"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5gFrJiXc74N"
      },
      "source": [
        "## 2. Multi Layer Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryD4Qg7fwq4Z"
      },
      "source": [
        "1. Feature extractor using word2vec and gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04qOD0u3wvxx"
      },
      "outputs": [],
      "source": [
        "class Preprocessor2:\n",
        "  def __init__(self):\n",
        "    df = pd.read_csv('SMSSpamCollection', sep=\"\\t\", names = [_CAT, _MSG],\n",
        "                     header = None)\n",
        "\n",
        "    # Replace dependent variable to a int value: 0 = not_spam, 1 = spam\n",
        "    df.category = df.category.map({'ham': 0., 'spam': 1.})\n",
        "    df.astype({_CAT:'float'})\n",
        "    self.df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # extract input and labels\n",
        "    X = self.df['message'].values\n",
        "    Y = self.df['category'].values\n",
        "    # create train/test split using sklearn\n",
        "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
        "        X, Y, test_size=0.7)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df[_MSG])\n",
        "\n",
        "  def __iter__(self):\n",
        "      for line in self.df[_MSG]:\n",
        "          # assume there's one document per line, tokens separated by whitespace\n",
        "          yield self.preprocess(line)\n",
        "\n",
        "  def preprocess(self, sentence):\n",
        "    return _preprocesar_oracion_2(sentence)\n",
        "\n",
        "\n",
        "def generate_model(corpus, num_features):\n",
        "  # Followed training recomendation from https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook\n",
        "  model = Word2Vec(vector_size=num_features, min_count=1, window=2,\n",
        "                  sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20,\n",
        "                  workers=4)\n",
        "  sentences = [s.split() for s in corpus]\n",
        "  model.build_vocab(sentences, progress_per=10000)\n",
        "  model.train(sentences, total_examples=len(sentences), epochs=30, report_delay=1)\n",
        "  return model\n",
        "\n",
        "\n",
        "def extract_features_dataset(model, preprocessed_dataset, max_length_words = 100,\n",
        "                             num_features = 20):\n",
        "  \"\"\"\"\n",
        "  retorne un tensor features_dataset con N×D dimensiones, donde N es la cantidad de\n",
        "  observaciones de la muestra, y D la cantidad de dimensiones seleccionadas\n",
        "  para el vector incrustado (num_features).\n",
        "  \"\"\"\n",
        "  N = len(preprocessed_dataset)\n",
        "\n",
        "  empty_vector = np.array([0]*num_features, dtype=np.float32)\n",
        "  features_list = []\n",
        "  for i in range(len(preprocessed_dataset)):\n",
        "    feature = []\n",
        "    sentence = preprocessed_dataset[i]\n",
        "    # print(sentence)\n",
        "    words = sentence.split()\n",
        "    for word in words:\n",
        "      try:\n",
        "        feature.append(model.wv.get_vector(word))\n",
        "      except KeyError:\n",
        "        feature.append(empty_vector)\n",
        "        print(f\"word not found ${word}, added empty embedding instead\")\n",
        "\n",
        "    # print(feature)  # each word has D dimensions\n",
        "    features_list.append(feature)\n",
        "\n",
        "  # normalize sentences to be of max_length_words size\n",
        "  features_list = keras.utils.pad_sequences(features_list, maxlen=max_length_words,\n",
        "                                            dtype='float32')\n",
        "  print(len(features_list))\n",
        "  return torch.tensor(features_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCTBXX0msQgw",
        "outputId": "3a9bd5f8-160d-4ee7-cb2d-9ce0d633e333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3901\n",
            "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.6404,  0.1182, -0.4312,  ..., -0.8848,  0.0256, -0.9930],\n",
            "         [-0.4023,  0.0924, -0.3467,  ..., -0.5513, -0.0664, -0.6518],\n",
            "         [-0.4219,  0.0732, -0.4652,  ..., -0.8297, -0.1072, -0.9690]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.4999,  0.0966, -0.4862,  ..., -0.6704, -0.1964, -0.8771],\n",
            "         [-0.6807,  0.0463, -0.6160,  ..., -0.7662, -0.2221, -1.0226],\n",
            "         [-0.3686,  0.0798, -0.2949,  ..., -0.4510, -0.1333, -0.6066]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.5623,  0.1100, -0.4421,  ..., -0.7003, -0.1947, -0.9612],\n",
            "         [-0.6659,  0.1389, -0.4566,  ..., -0.7896, -0.2062, -0.9776],\n",
            "         [-0.7198,  0.2948, -0.5315,  ..., -0.8216, -0.2765, -0.9982]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.2465,  0.0311, -0.2287,  ..., -0.2617, -0.0538, -0.4538],\n",
            "         [-0.6138,  0.0875, -0.5247,  ..., -0.7068, -0.2066, -1.0575],\n",
            "         [-0.5340,  0.2568, -0.3522,  ..., -0.6608, -0.3766, -1.2019]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.5807,  0.0875, -0.5073,  ..., -0.6964, -0.1934, -1.0435],\n",
            "         [-0.5341,  0.1456, -0.5675,  ..., -0.8081, -0.2541, -0.9839],\n",
            "         [-0.6148,  0.1034, -0.5438,  ..., -0.8326, -0.1540, -1.0595]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "         ...,\n",
            "         [-0.5919,  0.0799, -0.4742,  ..., -0.7156, -0.1459, -1.0622],\n",
            "         [-0.5472,  0.1630, -0.4118,  ..., -0.7103, -0.1128, -1.0727],\n",
            "         [-0.4959,  0.1006, -0.4363,  ..., -0.6494, -0.1774, -0.9631]]])\n",
            "torch.Size([3901, 100, 20])\n"
          ]
        }
      ],
      "source": [
        "D = 20  # embedding dimensions\n",
        "corpus = Preprocessor2()\n",
        "model = generate_model(corpus, D)\n",
        "\n",
        "sample = [corpus.preprocess(sentence) for sentence in corpus.x_test]\n",
        "\n",
        "features_dataset_20D = extract_features_dataset(model, sample,\n",
        "                                                max_length_words=100,\n",
        "                                                num_features=D)\n",
        "print(features_dataset_20D)\n",
        "print(features_dataset_20D.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25q0J9aKac5b",
        "outputId": "70c955a4-a5c3-44c2-8dab-a17817ce6fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1671\n",
            "3901\n",
            "1671\n",
            "3901\n"
          ]
        }
      ],
      "source": [
        "def create_data_loaders2(model, corpus, max_length_words=100, D=20, batch_size = 64):\n",
        "  raw_x_train =  [corpus.preprocess(sentence) for sentence in corpus.x_train]\n",
        "  raw_x_test =  [corpus.preprocess(sentence) for sentence in corpus.x_test]\n",
        "  y_train = corpus.y_train\n",
        "  y_test = corpus.y_test\n",
        "\n",
        "  #convert sequence of strings to tokens\n",
        "  x_train = extract_features_dataset(model, raw_x_train, num_features=D)\n",
        "  x_test = extract_features_dataset(model, raw_x_test, num_features=D)\n",
        "\n",
        "  #create data loaders\n",
        "  training_set = DatasetMaper(x_train, y_train)\n",
        "  test_set = DatasetMaper(x_test, y_test)\n",
        "  loader_training = DataLoader(training_set, batch_size=batch_size)\n",
        "  loader_test = DataLoader(test_set)\n",
        "  return loader_training, loader_test\n",
        "\n",
        "\n",
        "MAX_WORDS_PER_SENTENCE = 100\n",
        "D = 20  # embedding dimensions\n",
        "corpus = Preprocessor2()\n",
        "model20 = generate_model(corpus, D)\n",
        "loader_training_D20, loader_test_D20 = create_data_loaders2(model20, corpus, MAX_WORDS_PER_SENTENCE, D)\n",
        "\n",
        "D = 100  # embedding dimensions\n",
        "model100 = generate_model(corpus, D)\n",
        "loader_training_D100, loader_test_D100 = create_data_loaders2(model100, corpus, MAX_WORDS_PER_SENTENCE, D)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n1Vrun1dBor"
      },
      "source": [
        "2. MPL SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yagt5N1WdDqU"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def create_MLP_model(input_layer_size=2000, intermediate_layer_size=14, ouput_size=2):\n",
        "    # Model creation with neural net Sequential model\n",
        "    model=nn.Sequential(\n",
        "        nn.Linear(input_layer_size, intermediate_layer_size),   # 1 layer: _INPUT_LAYER features (pixels)\n",
        "        #nn.ReLU(),                                  # Defining Regular linear unit as activation\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(intermediate_layer_size, ouput_size),   # Out Layer\n",
        "        nn.LogSoftmax(dim=1) # Defining the log softmax to find the probablities for the last output unit\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_new_model(input_size=2000, intermediate_size=14):\n",
        "\n",
        "  print(\"Running on device: \", device)\n",
        "  mlp_model = create_MLP_model(input_size, intermediate_size)\n",
        "\n",
        "  #moving models to device\n",
        "  mlp_model.to(device)\n",
        "  #create error criterion\n",
        "  criterion = nn.NLLLoss()\n",
        "  print(\"MLP model\\n\", mlp_model)\n",
        "\n",
        "  return mlp_model, criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eurU7s7kec4D"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, trainloader, epochs = 15, lr = 0.01):\n",
        "    time0 = time()\n",
        "    running_loss_list= []\n",
        "    epochs_list = []\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    for e in range(epochs):\n",
        "        running_loss = 0\n",
        "\n",
        "        #go for every batch\n",
        "        for features, labels in trainloader:\n",
        "            #move data to specific device\n",
        "            features = features.flatten(1, 2)\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # defining gradient in each epoch as 0\n",
        "            optimizer.zero_grad()\n",
        "            # modeling for each image batch\n",
        "            features = features.float()\n",
        "            output = model(features)\n",
        "\n",
        "            labels = labels.long()\n",
        "            # calculating the loss\n",
        "            loss = criterion(output, labels)  # tener cuidado que tanto etiquetas como outputs esten en el formato que espera la func de error\n",
        "\n",
        "            # This is where the model learns by backpropagating\n",
        "            loss.backward()  # Calculo de los gradientes (Matrioshka mas grande).\n",
        "\n",
        "            # And optimizes its weights here\n",
        "            optimizer.step()  # Actualiza pesos en todas las capas\n",
        "\n",
        "            # calculating the loss\n",
        "            running_loss += loss.item()\n",
        "            # print(f'Output {output}, labels {labels}, loss {loss.item()}')\n",
        "            # print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
        "\n",
        "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5qx93Thee0M"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def test_model_mlp(testloader, model, input_size=2000):\n",
        "    correct_count, all_count = 0, 0\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "\n",
        "    for features, labels in testloader:\n",
        "      #move data to specific device\n",
        "      features = features.flatten(1, 2)\n",
        "      features = features.to(device)\n",
        "      features = features.float()\n",
        "      labels = labels.to(device)\n",
        "      labels = labels.long()\n",
        "\n",
        "      for i in range(len(labels)):\n",
        "        feature = features[i].view(1, input_size)\n",
        "\n",
        "        #evaluate model with no grad\n",
        "        with torch.no_grad():\n",
        "            logps = model(feature)\n",
        "\n",
        "        ps = torch.exp(logps)\n",
        "        probab = list(ps.cpu().numpy()[0])\n",
        "\n",
        "        #get predicted label\n",
        "        pred_label = probab.index(max(probab))\n",
        "        true_label = labels.cpu().numpy()[i]\n",
        "\n",
        "        all_predicted.append(pred_label)\n",
        "        all_labels.append(true_label)\n",
        "\n",
        "        if true_label == pred_label:\n",
        "          correct_count += 1\n",
        "\n",
        "        all_count += 1\n",
        "\n",
        "    # accuracy = correct_count/all_count\n",
        "    accuracy = accuracy_score(all_labels, all_predicted)\n",
        "    f1 = f1_score(all_labels, all_predicted, average='macro')\n",
        "\n",
        "    print(\"Number Of Observations Tested =\", all_count)\n",
        "    print(\"\\nModel Accuracy =\", accuracy)\n",
        "    print(\"\\nModel F1 =\", f1)\n",
        "\n",
        "    return accuracy, f1, all_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8kGgVxt4fUB"
      },
      "outputs": [],
      "source": [
        "def train_models(input_layer, intermediate_layer, loader_train, loader_test, trials=10):\n",
        "  accuracies = []\n",
        "  f1s = []\n",
        "  alpha = 0.01\n",
        "\n",
        "  for run in range(trials):\n",
        "    print(f\"Training MLP model #{run}\")\n",
        "    new_model, criterion = get_new_model(input_layer, intermediate_layer)\n",
        "    mlp_model = train_model(new_model, criterion, loader_train,\n",
        "                            epochs = 10, lr = alpha)\n",
        "    print(f\"Testing MLP model #{run}\")\n",
        "    accuracy, f1, all_count = test_model_mlp(loader_test, mlp_model, input_layer)\n",
        "    accuracies.append(accuracy)\n",
        "    f1s.append(f1)\n",
        "    print(\"------------------------------------------------------\")\n",
        "\n",
        "  return accuracies, f1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0DPKenreiA9",
        "outputId": "dc4e0aa4-a75e-459a-8678-dade524c5dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "D = 20\n",
            "Training MLP model #0\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.10327964623769124\n",
            "Testing MLP model #0\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9543706741861061\n",
            "\n",
            "Model F1 = 0.8972132419117996\n",
            "------------------------------------------------------\n",
            "Training MLP model #1\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.11187851428985596\n",
            "Testing MLP model #1\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9556523968213279\n",
            "\n",
            "Model F1 = 0.9003578196550551\n",
            "------------------------------------------------------\n",
            "Training MLP model #2\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.10321795543034872\n",
            "Testing MLP model #2\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9502691617533966\n",
            "\n",
            "Model F1 = 0.8902334240713279\n",
            "------------------------------------------------------\n",
            "Training MLP model #3\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.1038234035174052\n",
            "Testing MLP model #3\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9515508843886183\n",
            "\n",
            "Model F1 = 0.8918835492346318\n",
            "------------------------------------------------------\n",
            "Training MLP model #4\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.1026903788248698\n",
            "Testing MLP model #4\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.952063573442707\n",
            "\n",
            "Model F1 = 0.8937490869468391\n",
            "------------------------------------------------------\n",
            "Training MLP model #5\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.10226551691691081\n",
            "Testing MLP model #5\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9548833632401948\n",
            "\n",
            "Model F1 = 0.8988900788054652\n",
            "------------------------------------------------------\n",
            "Training MLP model #6\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.10133658250172933\n",
            "Testing MLP model #6\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9543706741861061\n",
            "\n",
            "Model F1 = 0.8972132419117996\n",
            "------------------------------------------------------\n",
            "Training MLP model #7\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.10212186972300212\n",
            "Testing MLP model #7\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9538579851320175\n",
            "\n",
            "Model F1 = 0.8969443466009158\n",
            "------------------------------------------------------\n",
            "Training MLP model #8\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.10218571424484253\n",
            "Testing MLP model #8\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9546270187131505\n",
            "\n",
            "Model F1 = 0.8977025797142718\n",
            "------------------------------------------------------\n",
            "Training MLP model #9\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=2000, out_features=1400, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=1400, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 0.1032791018486023\n",
            "Testing MLP model #9\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9543706741861061\n",
            "\n",
            "Model F1 = 0.8972132419117996\n",
            "------------------------------------------------------\n",
            "------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"------------------------------------------------------\")\n",
        "print(\"D = 20\")\n",
        "input_size = 20 * MAX_WORDS_PER_SENTENCE\n",
        "accuracies_1, f1s_1 = train_models(input_size, int(input_size*.7),\n",
        "                                   loader_training_D20, loader_test_D20)\n",
        "\n",
        "print(\"------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE61n7T93Oi3",
        "outputId": "68cebbb3-07cf-421b-9a59-2bb295e4ab39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D = 14 \n",
            " Accuracies: [0.9543706741861061, 0.9556523968213279, 0.9502691617533966, 0.9515508843886183, 0.952063573442707, 0.9548833632401948, 0.9543706741861061, 0.9538579851320175, 0.9546270187131505, 0.9543706741861061]\n",
            "F1 scores: [0.8972132419117996, 0.9003578196550551, 0.8902334240713279, 0.8918835492346318, 0.8937490869468391, 0.8988900788054652, 0.8972132419117996, 0.8969443466009158, 0.8977025797142718, 0.8972132419117996]\n",
            "Accuracy ->\t mean = 0.9536016406049731, Std = 0.0016253132513865415\n",
            "F1 ->\t\t mean = 0.8961400610763907, Std = 0.0030102921288484913\n",
            "------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(f\"D = 14 \\n Accuracies: {accuracies_1}\\nF1 scores: {f1s_1}\")\n",
        "print(f\"Accuracy ->\\t mean = {np.mean(accuracies_1)}, Std = {np.std(accuracies_1)}\")\n",
        "print(f\"F1 ->\\t\\t mean = {np.mean(f1s_1)}, Std = {np.std(f1s_1)}\")\n",
        "print(\"------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoiI_sJazYXF",
        "outputId": "4a4e1193-086a-4201-c51a-940336a77811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "D = 100\n",
            "Training MLP model #0\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.515516432126363\n",
            "Testing MLP model #0\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8692642912073827\n",
            "\n",
            "Model F1 = 0.46503017004936914\n",
            "------------------------------------------------------\n",
            "Training MLP model #1\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.660879675547282\n",
            "Testing MLP model #1\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8700333247885158\n",
            "\n",
            "Model F1 = 0.47106951996078367\n",
            "------------------------------------------------------\n",
            "Training MLP model #2\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.731350028514862\n",
            "Testing MLP model #2\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8936170212765957\n",
            "\n",
            "Model F1 = 0.628193335836672\n",
            "------------------------------------------------------\n",
            "Training MLP model #3\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.6719121734301248\n",
            "Testing MLP model #3\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8692642912073827\n",
            "\n",
            "Model F1 = 0.46503017004936914\n",
            "------------------------------------------------------\n",
            "Training MLP model #4\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.6968838969866433\n",
            "Testing MLP model #4\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.9279671879005383\n",
            "\n",
            "Model F1 = 0.7991763191763191\n",
            "------------------------------------------------------\n",
            "Training MLP model #5\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.606636651357015\n",
            "Testing MLP model #5\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8692642912073827\n",
            "\n",
            "Model F1 = 0.46503017004936914\n",
            "------------------------------------------------------\n",
            "Training MLP model #6\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.7108816663424173\n",
            "Testing MLP model #6\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8746475262753141\n",
            "\n",
            "Model F1 = 0.5059212862489869\n",
            "------------------------------------------------------\n",
            "Training MLP model #7\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.684127565224965\n",
            "Testing MLP model #7\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8723404255319149\n",
            "\n",
            "Model F1 = 0.48878520904382977\n",
            "------------------------------------------------------\n",
            "Training MLP model #8\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.748920790354411\n",
            "Testing MLP model #8\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8310689566777749\n",
            "\n",
            "Model F1 = 0.7279100975561792\n",
            "------------------------------------------------------\n",
            "Training MLP model #9\n",
            "Running on device:  cpu\n",
            "MLP model\n",
            " Sequential(\n",
            "  (0): Linear(in_features=10000, out_features=7000, bias=True)\n",
            "  (1): Sigmoid()\n",
            "  (2): Linear(in_features=7000, out_features=2, bias=True)\n",
            "  (3): LogSoftmax(dim=1)\n",
            ")\n",
            "\n",
            "Training Time (in minutes) = 2.7920812726020814\n",
            "Testing MLP model #9\n",
            "Number Of Observations Tested = 3901\n",
            "\n",
            "Model Accuracy = 0.8692642912073827\n",
            "\n",
            "Model F1 = 0.46503017004936914\n",
            "------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "print(\"D = 100\")\n",
        "input_size = 100 * MAX_WORDS_PER_SENTENCE\n",
        "\n",
        "accuracies_2, f1s_2 = train_models(input_size, int(input_size*.7),\n",
        "                                   loader_training_D100, loader_test_D100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mkuc0Rnfzb2Y",
        "outputId": "09e07449-e04e-4d98-c779-6952c9ede789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------\n",
            "D = 70 \n",
            " Accuracies: [0.8692642912073827, 0.8700333247885158, 0.8936170212765957, 0.8692642912073827, 0.9279671879005383, 0.8692642912073827, 0.8746475262753141, 0.8723404255319149, 0.8310689566777749, 0.8692642912073827]\n",
            "F1 scores: [0.46503017004936914, 0.47106951996078367, 0.628193335836672, 0.46503017004936914, 0.7991763191763191, 0.46503017004936914, 0.5059212862489869, 0.48878520904382977, 0.7279100975561792, 0.46503017004936914]\n",
            "Accuracy ->\t mean = 0.8746731607280184, Std = 0.02290075034084115\n",
            "F1 ->\t\t mean = 0.5481176448020249, Std = 0.11858300819821961\n"
          ]
        }
      ],
      "source": [
        "print(\"------------------------------------------------------\")\n",
        "print(f\"D = 70 \\n Accuracies: {accuracies_2}\\nF1 scores: {f1s_2}\")\n",
        "print(f\"Accuracy ->\\t mean = {np.mean(accuracies_2)}, Std = {np.std(accuracies_2)}\")\n",
        "print(f\"F1 ->\\t\\t mean = {np.mean(f1s_2)}, Std = {np.std(f1s_2)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}